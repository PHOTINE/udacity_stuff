{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imitation Learning: Behavioural Cloning and the DAGGER Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import the Necessary Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add parent dir to find package. Only needed for source code build, pip install doesn't need it.\n",
    "import os, inspect\n",
    "currentdir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "parentdir = os.path.dirname(os.path.dirname(currentdir))\n",
    "os.sys.path.insert(0,parentdir)\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import pybullet_envs\n",
    "import os.path\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Instantiate the Environment, Agent, and Expert Demonstrator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WalkerBase::__init__ start\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jamesough/gym/gym/logger.py:30: UserWarning: \u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/Users/jamesough/gym/gym/logger.py:30: UserWarning: \u001b[33mWARN: Environment '<class 'pybullet_envs.gym_locomotion_envs.HumanoidFlagrunBulletEnv'>' has deprecated methods '_step' and '_reset' rather than 'step' and 'reset'. Compatibility code invoked. Set _gym_disable_underscore_compat = True to disable this behavior.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "from flagrun_expert_demonstrator import *\n",
    "\n",
    "gui = True\n",
    "env = gym.make(\"HumanoidFlagrunBulletEnv-v0\")\n",
    "if (gui):\n",
    "  env.render(mode=\"human\")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as f \n",
    "                            \n",
    "class StudentPolicy(nn.Module):\n",
    "    \"Simple multi-layer perceptron policy, no internal state\"\n",
    "    def __init__(self, observation_space, action_space):\n",
    "        super(StudentPolicy, self).__init__()\n",
    "        self.weights_dense1 = nn.Linear(observation_space.shape[0], 256) \n",
    "        self.weights_dense2 = nn.Linear(256, 128) \n",
    "        self.weights_dense_final = nn.Linear(128, action_space.shape[0]) \n",
    "\n",
    "        torch.nn.init.xavier_uniform_(self.weights_dense1.weight)\n",
    "        torch.nn.init.xavier_uniform_(self.weights_dense2.weight)\n",
    "        torch.nn.init.xavier_uniform_(self.weights_dense_final.weight)\n",
    "        \n",
    "        self.weights_dense1.bias.data.fill_(0.01)\n",
    "        self.weights_dense2.bias.data.fill_(0.01)\n",
    "        self.weights_dense_final.bias.data.fill_(0.01)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = f.relu(self.weights_dense1(x))\n",
    "        x = f.relu(self.weights_dense2(x))\n",
    "        x = self.weights_dense_final(x)\n",
    "        return x\n",
    "\n",
    "                            \n",
    "        \n",
    "def rollout_for_one_episode(policy= ExpertPolicy(env.observation_space, env.action_space)):\n",
    "    '''\n",
    "    \n",
    "    Rollout a particular policy for a single episode.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    rollout_data = {'observations':[], 'actions':[]}\n",
    "    pi = policy\n",
    "    \n",
    "    frame = 0\n",
    "    score = 0\n",
    "    restart_delay = 0\n",
    "    obs = env.reset()\n",
    "    from itertools import count\n",
    "    for t in count():\n",
    "        rollout_data['observations'].append(obs)\n",
    "        a = pi(torch.Tensor(obs)).data.numpy()\n",
    "        import pdb\n",
    "        rollout_data['actions'].append(a)\n",
    "        obs, r, done, _ = env.step(a)\n",
    "        score += r\n",
    "        frame += 1\n",
    "        if (gui):\n",
    "          time.sleep(1./60)\n",
    "\n",
    "        still_open = env.render(\"human\")\n",
    "\n",
    "        if still_open==False:\n",
    "            return\n",
    "        if not done: continue\n",
    "        if restart_delay==0:\n",
    "            print(\"score=%0.2f in %i frames\" % (score, frame))\n",
    "            if still_open!=True:      # not True in multiplayer or non-Roboschool environment\n",
    "                break\n",
    "            restart_delay = 60*2  # 2 sec at 60 fps\n",
    "        restart_delay -= 1\n",
    "        if restart_delay==0: break\n",
    "    return rollout_data\n",
    "\n",
    "\n",
    "def rollout_for_n_episodes(n, policy= ExpertPolicy(env.observation_space, env.action_space)):\n",
    "    '''\n",
    "    Rollout a particular policy for a n episodes.\n",
    "    \n",
    "    '''\n",
    "    rollout_data = {'observations':[], 'actions':[]}\n",
    "    for i in range(n):\n",
    "        print('episode', i)\n",
    "        recent_rollout_data = rollout_for_one_episode(policy)\n",
    "        rollout_data['observations'].extend(recent_rollout_data['observations'])\n",
    "        rollout_data['actions'].extend(recent_rollout_data['actions'])\n",
    "    return rollout_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train the Agent with Behavioural Cloning\n",
    "\n",
    "Press 'w' in the pybullet GUI to turn on wireframe mode to render more quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rolling Out Expert\n",
      "episode 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jamesough/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:64: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score=655.09 in 1000 frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jamesough/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:69: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 1\n",
      "score=913.55 in 1000 frames\n",
      "episode 2\n",
      "score=-413.89 in 1000 frames\n",
      "episode 3\n",
      "score=1197.03 in 1000 frames\n",
      "episode 4\n",
      "score=657.30 in 1000 frames\n",
      "episode 5\n",
      "score=483.65 in 1000 frames\n",
      "episode 6\n",
      "score=-108.99 in 1000 frames\n",
      "episode 7\n",
      "score=504.91 in 1000 frames\n",
      "episode 8\n",
      "score=-55.65 in 58 frames\n",
      "episode 9\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "Not connected to physics server.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-d4a9bd5670eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0mstudent_policy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStudentPolicy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation_space\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m \u001b[0mbehavioural_cloning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpert_policy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstudent_policy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-d4a9bd5670eb>\u001b[0m in \u001b[0;36mbehavioural_cloning\u001b[0;34m(expert_policy, student_policy)\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Rolling Out Expert'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m     \u001b[0mexpert_rollout_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrollout_for_n_episodes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpert_policy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m     \u001b[0;31m# train initial student model with behavioural cloning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0mstudent_policy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudent_policy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpert_rollout_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-5c56158c2434>\u001b[0m in \u001b[0;36mrollout_for_n_episodes\u001b[0;34m(n, policy)\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'episode'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m         \u001b[0mrecent_rollout_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrollout_for_one_episode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m         \u001b[0mrollout_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'observations'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecent_rollout_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'observations'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0mrollout_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'actions'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecent_rollout_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'actions'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-5c56158c2434>\u001b[0m in \u001b[0;36mrollout_for_one_episode\u001b[0;34m(policy)\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mrollout_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'actions'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m         \u001b[0mscore\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mframe\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gym/gym/wrappers/time_limit.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_episode_started_at\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Cannot call env.step() before calling reset()\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pybullet_envs/gym_locomotion_envs.py\u001b[0m in \u001b[0;36m_step\u001b[0;34m(self, a)\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscene\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiplayer\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# if multiplayer, action first applied to all robots, then global step() called, then _step() for all robots with the same actions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrobot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscene\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pybullet_envs/robot_locomotors.py\u001b[0m in \u001b[0;36mapply_action\u001b[0;34m(self, a)\u001b[0m\n\u001b[1;32m    186\u001b[0m                 \u001b[0mforce_gain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpower\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m17\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmotors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmotor_power\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m                         \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_motor_torque\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforce_gain\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mpower\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpower\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0malive_bonus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpitch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pybullet_envs/robot_bases.py\u001b[0m in \u001b[0;36mset_motor_torque\u001b[0;34m(self, torque)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mset_motor_torque\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorque\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# just some synonyme method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_torque\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorque\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mset_torque\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorque\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pybullet_envs/robot_bases.py\u001b[0m in \u001b[0;36mset_torque\u001b[0;34m(self, torque)\u001b[0m\n\u001b[1;32m    328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mset_torque\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorque\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 330\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_p\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetJointMotorControl2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbodyIndex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbodies\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbodyIndex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjointIndex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjointIndex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontrolMode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpybullet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTORQUE_CONTROL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorque\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#, positionGain=0.1, velocityGain=0.1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    331\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mreset_current_position\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvelocity\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# just some synonyme method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31merror\u001b[0m: Not connected to physics server."
     ]
    }
   ],
   "source": [
    "from torch.utils import data\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as f\n",
    "\n",
    "class Dataset(data.Dataset):\n",
    "  'Characterizes a dataset for PyTorch'\n",
    "  def __init__(self, X, Y):\n",
    "        'Initialization'\n",
    "        self.X=X\n",
    "        self.Y=Y\n",
    "\n",
    "  def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return len(self.X)\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "        # Select sample\n",
    "        X = self.X[index]\n",
    "        Y = self.Y[index]\n",
    "        return X, Y\n",
    "\n",
    "def train_model(policy, training_data):\n",
    "    '''\n",
    "    Given a dict of training data, train a policy network\n",
    "    using supervised learning.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    dataset = Dataset(training_data['observations'], training_data['actions'])\n",
    "    dataloader = data.DataLoader(dataset, batch_size = 128)\n",
    "    \n",
    "    optimizer = optim.Adam(policy.parameters(), lr=1e-3)\n",
    "    mse = nn.MSELoss()\n",
    "    for ne in range(100):\n",
    "        for obs, act in dataloader:\n",
    "\n",
    "            obs = Variable(obs)\n",
    "            act = Variable(act)\n",
    "\n",
    "            policy.zero_grad()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            predicted_action = policy(obs)\n",
    "            loss = mse(predicted_action, act.float())\n",
    "\n",
    "            loss.backward(retain_graph=True)\n",
    "            optimizer.step()\n",
    "\n",
    "            print(\"Epoch: {}, Total loss: {}\".format(ne, loss))\n",
    "    return policy\n",
    "            \n",
    "def evaluate_model(policy, data):\n",
    "    '''\n",
    "    Evaluate a policy on a list of recorded observations.\n",
    "    '''\n",
    "    actions = []\n",
    "    for obs in data:\n",
    "        obs = Variable(torch.Tensor(obs))\n",
    "        predicted_action = policy(obs)\n",
    "        actions.append(predicted_action.data.numpy())\n",
    "    return actions\n",
    "\n",
    "def behavioural_cloning(expert_policy, student_policy):\n",
    "    '''\n",
    "    Given an expert demonstrator and a student policy, perform\n",
    "    n iterations of dagger.\n",
    "    \n",
    "    '''\n",
    "    # collect initial expert demonstrations\n",
    "    n=10\n",
    "    print('Rolling Out Expert')\n",
    "    expert_rollout_data = rollout_for_n_episodes(10, expert_policy)\n",
    "    # train initial student model with behavioural cloning\n",
    "    student_policy = train_model(student_policy, expert_rollout_data)\n",
    "    return student_policy\n",
    "\n",
    "\n",
    "expert_policy= ExpertPolicy(env.observation_space, env.action_space)\n",
    "student_policy = StudentPolicy(env.observation_space, env.action_space)   \n",
    "\n",
    "behavioural_cloning(expert_policy, student_policy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_rollout_data = rollout_for_n_episodes(10, student_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train the Agent with the DAGGER Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dagger(expert_policy, student_policy, n_dagger_iterations):\n",
    "    '''\n",
    "    Given an expert demonstrator and a student policy, perform\n",
    "    n iterations of dagger.\n",
    "    \n",
    "    '''\n",
    "    # collect initial expert demonstrations\n",
    "    n=10\n",
    "    expert_rollout_data = rollout_for_n_episodes(n, expert_policy)\n",
    "    # train initial student model with behavioural cloning\n",
    "    trained_student = train_model(student_policy, expert_rollout_data)\n",
    "    \n",
    "    for i in range(n_dagger_iterations):\n",
    "        # rollout student model\n",
    "        student_rollout_data = rollout_for_n_episodes(3, student_policy)\n",
    "        # evaluate expert actions on student's trajectories and add to dataset\n",
    "        expert_corrections = evaluate_model(expert_policy, student_rollout_data['observations'])\n",
    "        training_data = {'observations': expert_rollout_data['observations'] + student_rollout_data['observations'],\n",
    "                         'actions':      expert_rollout_data['actions']      + expert_corrections}\n",
    "        # train student model with behavioural cloning\n",
    "        student_policy =  train_model(student_policy, training_data)\n",
    "        \n",
    "    return student_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expert_policy= ExpertPolicy(env.observation_space, env.action_space)\n",
    "student_policy = StudentPolicy(env.observation_space, env.action_space)   \n",
    "dagger(expert_policy, student_policy, n_dagger_iterations = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Explore\n",
    "\n",
    "In this exercise, we have implemented the behavioural cloning and DAGGER algorithms, and demonstrated how to use them to solve a pybullet Gym environment. To continue your learning, you are encouraged to complete any (or all!) of the following tasks:\n",
    "\n",
    "- Plot the  behavioural cloning student policy's average reward for a variety of numbers of episodes of expert data, and compare to the expert.\n",
    "- Change the environment to 'HumanoidFlagrunHarderBulletEnv-v0' environment, and run behavioural cloning on 10 episodes of expert data. Watch the visualization. Does behavioural cloning work better than for previous environment? Why?\n",
    "- Try and reduce the amount of expert data needed for Dagger to work on 'HumanoidFlagrunHarderBulletEnv-v0'. Can you reach an average reward of 500 over ten episodes, using only a total of 100 frames of expert data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "solutions: \n",
    "\n",
    "\n",
    "- reward increases slowly...\n",
    "- Works better because the blocks push it off-distribution so you widen the expert's trajectory distribution (it knows how to correct).\n",
    "- trick is to (1) run loads of iterations of dagger, (2) use temporally-distant examples."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
