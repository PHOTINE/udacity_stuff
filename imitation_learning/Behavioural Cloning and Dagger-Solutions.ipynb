{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imitation Learning: Behavioural Cloning and the DAGGER Algorithm\n",
    "\n",
    "\n",
    "In this lesson you'll use imitation learning to teach a student policy to mimic an expert demonstrator! This is an important technique in robotics research.\n",
    "\n",
    "We'll consider two basic approaches to imitation learning that don't require intensive RL algotithms like you have used in previous sections. These only work when you have direct access to the observation space and action space of an expert demonstrator (e.g. recorded commands from a car's data bus as a human demonstrator drives!).\n",
    "\n",
    "We'll first try the behavioural cloning technique, which is a simple baseline for imitation learning. It can generate good policies, but they typically can't recover after making mistakes.\n",
    "\n",
    "We'll then try the DAGGER algorithm, which results in policies that can recover from their mistakes!\n",
    "\n",
    "You can then try the exercises at the end.\n",
    "\n",
    "\n",
    "1. Setup\n",
    "2. View the student and expert policies\n",
    "3. Run Behavioural Cloning\n",
    "4. Run the DAGGER algorithm\n",
    "5. Exercises!\n",
    "\n",
    "**This notebook doesn't need a GPU! You should be able to run it on a laptop CPU.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Import the Necessary Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x113395850>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#add parent dir to find package. Only needed for source code build, pip install doesn't need it.\n",
    "import os, inspect\n",
    "currentdir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "parentdir = os.path.dirname(os.path.dirname(currentdir))\n",
    "os.sys.path.insert(0,parentdir)\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import pybullet_envs\n",
    "import pybullet as p\n",
    "import os.path\n",
    "import time\n",
    "import torch\n",
    "torch.manual_seed(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Instantiate the Environment and Expert Demonstrator\n",
    "\n",
    "We have two versions of the environment:\n",
    "\n",
    "- `env_flagrun_with_rendering` which has a GUI for visualization\n",
    "- `env_flagrun_without_rendering` which runs very quickly but doesn't show any visualization.\n",
    "\n",
    "These will be useful later!\n",
    "\n",
    "**If you ever get an error from the physics server, you'll have to run this cell again.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WalkerBase::__init__ start\n",
      "WalkerBase::__init__ start\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jamesough/gym/gym/logger.py:30: UserWarning: \u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "from utils import rollout_for_one_episode as rollout_for_one_episode\n",
    "from utils import rollout_for_n_episodes as rollout_for_n_episodes\n",
    "\n",
    "# shutdown any physics clients that already exist\n",
    "try: p.disconnect()\n",
    "except: pass\n",
    "\n",
    "# build the two versions of the environment\n",
    "env_flagrun_with_rendering = gym.make(\"HumanoidFlagrunBulletEnv-v0\")\n",
    "env_flagrun_with_rendering.render(mode=\"human\")\n",
    "env_flagrun_without_rendering = gym.make(\"HumanoidFlagrunBulletEnv-v0\")\n",
    "\n",
    "\n",
    "# instantiate the expert\n",
    "from flagrun_expert_demonstrator import *\n",
    "flagrun_expert = ExpertPolicy(env_flagrun_with_rendering.observation_space,\n",
    "                                    env_flagrun_with_rendering.action_space)\n",
    "           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Watch the Untrained Student Policy\n",
    "\n",
    "Our student policy is a two-layer neural net.\n",
    "\n",
    "We'll use the first version of the environment, `env_flagrun_with_rendering`, which runs in real-time and creates a GUI for visualization.\n",
    "\n",
    "Watch few rollouts in the GUI: the humanoid will fall to the floor because the policy hasn't been trained yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 0\n",
      "score=67.09 in 39 frames\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-529797bce42a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m                        \u001b[0mpolicy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstudent_policy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m                        \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv_flagrun_with_rendering\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m                        render=True)\n\u001b[0m",
      "\u001b[0;32m~/misc/udacity/imitation_learning/utils.py\u001b[0m in \u001b[0;36mrollout_for_n_episodes\u001b[0;34m(n, policy, env, render)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'episode'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0mrecent_rollout_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrollout_for_one_episode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0mrollout_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'observations'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecent_rollout_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'observations'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mrollout_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'actions'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecent_rollout_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'actions'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/misc/udacity/imitation_learning/utils.py\u001b[0m in \u001b[0;36mrollout_for_one_episode\u001b[0;34m(policy, env, render)\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mrollout_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'actions'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0mscore\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mframe\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gym/gym/wrappers/time_limit.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_episode_started_at\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Cannot call env.step() before calling reset()\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pybullet_envs/gym_locomotion_envs.py\u001b[0m in \u001b[0;36m_step\u001b[0;34m(self, a)\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscene\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiplayer\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# if multiplayer, action first applied to all robots, then global step() called, then _step() for all robots with the same actions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrobot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscene\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pybullet_envs/robot_locomotors.py\u001b[0m in \u001b[0;36mapply_action\u001b[0;34m(self, a)\u001b[0m\n\u001b[1;32m    186\u001b[0m                 \u001b[0mforce_gain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpower\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m17\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmotors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmotor_power\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m                         \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_motor_torque\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforce_gain\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mpower\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpower\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0malive_bonus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpitch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# instantiate an untrained student policy\n",
    "from model import StudentPolicy as StudentPolicy\n",
    "student_policy = StudentPolicy(env_flagrun_with_rendering.observation_space,\n",
    "                               env_flagrun_with_rendering.action_space)  \n",
    "\n",
    "\n",
    "# view the untrained student policy (it should flop on the floor!)\n",
    "rollout_data = rollout_for_n_episodes(n = 3,\n",
    "                       policy = student_policy,\n",
    "                       env = env_flagrun_with_rendering,\n",
    "                       render=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Evaluate the Untrained Student Policy\n",
    "\n",
    "\n",
    "Let's run the untrained student ten more times, recording the reward so that we have a baseline for later.\n",
    "\n",
    "We'll use the second version of the environment, `env_flagrun_without_rendering`, which runs very quickly but doesn't show any visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rollout_data = rollout_for_n_episodes(n = 10,\n",
    "                                      policy = student_policy,\n",
    "                                      env = env_flagrun_without_rendering,\n",
    "                                      render=False)\n",
    "\n",
    "mean_student_score = np.mean(rollout_data['scores'])\n",
    "\n",
    "print('Average Expert Score:', mean_student_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We ran the untrained student policy for 1000 iterations and recorded the scores so you can get a less-noisy idea of the score:\n",
    "\n",
    "![](student_score_histogram.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Watch the Expert Demonstrator\n",
    "\n",
    "Now we'll visualize the expert demonstrator!\n",
    "\n",
    "You should be able to observe three distinct behaviours:\n",
    "\n",
    "- Running towards a target\n",
    "- Changing direction\n",
    "- Getting up after a fall\n",
    "\n",
    "Later, we'll train the student policy to imitate the expert.\n",
    "\n",
    "CTRL+drag in the GUI to rotate the view. You can click+drag on the expert to knock it over and see how it recovers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flagrun_expert = ExpertPolicy(env_flagrun_with_rendering.observation_space,\n",
    "                                    env_flagrun_with_rendering.action_space)\n",
    "           \n",
    "rollout_data = rollout_for_n_episodes(1,\n",
    "                       flagrun_expert,\n",
    "                       env = env_flagrun_with_rendering,\n",
    "                       render=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Evaluate the Expert Demonstrator\n",
    "\n",
    "Let's run the expert ten more times, recording the reward so that we have a baseline for later.\n",
    "\n",
    "We'll use the second version of the environment, `env_flagrun_without_rendering`, which runs very quickly but doesn't show any visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rollout_data = rollout_for_n_episodes(n = 10,\n",
    "                                      policy = ExpertPolicy(env_flagrun_without_rendering.observation_space,\n",
    "                                      env_flagrun_without_rendering.action_space),\n",
    "                                      env = env_flagrun_without_rendering,\n",
    "                                      render=False)\n",
    "\n",
    "mean_expert_score = np.mean(rollout_data['scores'])\n",
    "std_expert_score = np.std(rollout_data['scores'])\n",
    "\n",
    "print('Average Expert Score:', mean_expert_score, 'Standard Deviation in Expert Score:', std_expert_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We ran the expert policy for 1000 iterations so you get a clearer picture:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We ran the untrained student policy for 1000 iterations so you get a less-noisy idea of the score:\n",
    "\n",
    "![](expert_score_histogram.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll aim to hit an average score of about 500 with our student policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Train the Student Policy with Behavioural Cloning\n",
    "\n",
    "In behavioural cloning, we run the expert policy and record all the *[state, action]* pairs. We then train a student policy (with supervised learning!) to directly imitate the expert's actions.\n",
    "\n",
    "We've provided a helper function, `train_model`, which will train the student policy with the recorded expert *[state, action]* pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import train_model as train_model\n",
    "from utils import Dataset as Dataset\n",
    "\n",
    "def behavioural_cloning(expert_policy, student_policy):\n",
    "    '''\n",
    "    Given an expert demonstrator and a student policy, perform\n",
    "    n iterations of dagger.\n",
    "    \n",
    "    '''\n",
    "    # collect initial expert demonstrations\n",
    "    n=10\n",
    "    print('Rolling Out Expert')\n",
    "    expert_rollout_data = rollout_for_n_episodes(10,\n",
    "                       expert_policy,\n",
    "                       env = env_flagrun_without_rendering,\n",
    "                       render=False)\n",
    "    \n",
    "    # train student policy with supervised learning\n",
    "    print('Training Student Model')\n",
    "    student_policy = train_model(student_policy, expert_rollout_data, num_epochs = 300)\n",
    "    return student_policy\n",
    "\n",
    "\n",
    "student_policy = StudentPolicy(env_flagrun_with_rendering.observation_space,\n",
    "                               env_flagrun_with_rendering.action_space)  \n",
    "\n",
    "# Now run behavioural cloning\n",
    "behavioural_cloning(flagrun_expert, student_policy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Watch the Trained Student Policy\n",
    "\n",
    "Note that behavioural cloning works suprisingly well! The student policy should be able to run and turn. \n",
    "\n",
    "However if the student falls over, it can't get back up, because the expert doesn't fall enough to produce much training data!\n",
    "\n",
    "The expert is too good to fail so the student never learns how to recover."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rollout_for_n_episodes(n = 5,\n",
    "                       policy = student_policy,\n",
    "                       env = env_flagrun_with_rendering,\n",
    "                       render=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Evaluate the Trained Student Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view the trained student policy (it should run, but fall over!)\n",
    "\n",
    "student_rollout_data = rollout_for_n_episodes(n=10,\n",
    "                                            policy = student_policy,\n",
    "                                            env = env_flagrun_without_rendering,\n",
    "                                            render=False)\n",
    "\n",
    "mean_student_score = np.mean(student_rollout_data['scores'])\n",
    "std_student_score = np.std(student_rollout_data['scores'])\n",
    "print('Average Expert Score:', mean_student_score, 'Standard Deviation in Expert Score:', std_student_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We ran the behavioural cloning-trained student policy for 1000 iterations so you get a less noisy idea of the score:\n",
    "\n",
    "![](student_bc_score_histogram.png)\n",
    "\n",
    "Compare this to the expert!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Train the Agent with the DAGGER Algorithm\n",
    "\n",
    "How can we train the student to get back up when it falls?\n",
    "\n",
    "One way is to ask the expert *what it would have done*, and then train on that data.\n",
    "\n",
    "This is the essence of the DAGGER algorithm!\n",
    "\n",
    "In the DAGGER algorithm, we first train a student policy with behavioural cloning. We then rollout this trained student policy and record all the states it visits. We then run these states through the expert policy (asking the expert what it *would have done*), generating expert actions, and use these new [state, expert_action] pairs as extra training data for another iteration of behavioural cloning. We can repeat this algorithm many times.\n",
    "\n",
    "We've provided a helper function, train_model, which will train the student policy with the recorded expert [state, action] pairs.\n",
    "\n",
    "The first iteration should give the same result as behavioural cloning.\n",
    "\n",
    "By the final iteration, the agent should be able to stand up when it falls!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rolling Out Expert\n",
      "episode 0\n",
      "score=1301.41 in 1000 frames\n",
      "episode 1\n",
      "score=629.01 in 1000 frames\n",
      "episode 2\n",
      "score=543.10 in 1000 frames\n",
      "episode 3\n",
      "score=676.28 in 1000 frames\n",
      "episode 4\n",
      "score=904.47 in 1000 frames\n",
      "episode 5\n",
      "score=416.10 in 1000 frames\n",
      "episode 6\n",
      "score=633.28 in 1000 frames\n",
      "episode 7\n",
      "score=822.85 in 1000 frames\n",
      "episode 8\n",
      "score=499.05 in 1000 frames\n",
      "episode 9\n",
      "score=539.21 in 1000 frames\n",
      "Training Student\n",
      "Epoch: 50, Total loss: 0.02372540906071663\n",
      "Iteration 0 of DAGGER\n",
      "episode 0\n",
      "score=148.70 in 89 frames\n",
      "episode 1\n",
      "score=-40.43 in 24 frames\n",
      "episode 2\n",
      "score=94.32 in 52 frames\n",
      "episode 3\n",
      "score=-602.38 in 117 frames\n",
      "episode 4\n",
      "score=80.15 in 57 frames\n",
      "episode 5\n",
      "score=575.75 in 412 frames\n",
      "episode 6\n",
      "score=169.31 in 91 frames\n",
      "episode 7\n",
      "score=129.13 in 75 frames\n",
      "episode 8\n",
      "score=30.42 in 68 frames\n",
      "episode 9\n",
      "score=832.12 in 258 frames\n",
      "Epoch: 50, Total loss: 0.36704450845718384\n",
      "Epoch: 100, Total loss: 0.1790788620710373\n",
      "Epoch: 150, Total loss: 0.1599975973367691\n",
      "Epoch: 200, Total loss: 0.15275225043296814\n",
      "Epoch: 250, Total loss: 0.154378280043602\n",
      "Epoch: 300, Total loss: 0.07059424370527267\n",
      "Epoch: 350, Total loss: 0.11388439685106277\n",
      "Epoch: 400, Total loss: 0.07414469122886658\n",
      "Epoch: 450, Total loss: 0.0907498225569725\n",
      "Epoch: 500, Total loss: 0.07661131024360657\n",
      "Epoch: 550, Total loss: 0.059681400656700134\n",
      "Epoch: 600, Total loss: 0.0728178545832634\n",
      "Epoch: 650, Total loss: 0.04924808815121651\n",
      "Epoch: 700, Total loss: 0.0813651904463768\n",
      "Epoch: 750, Total loss: 0.046863142400979996\n",
      "Epoch: 800, Total loss: 0.0770433321595192\n",
      "Epoch: 850, Total loss: 0.06011521443724632\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from utils import train_model as train_model\n",
    "\n",
    "def evaluate_expert(policy, data):\n",
    "    '''\n",
    "    Evaluate an expert policy on a list of recorded observations.\n",
    "    What would it have done?\n",
    "    '''\n",
    "    actions = []\n",
    "    for obs in data:\n",
    "        obs = Variable(torch.Tensor(obs))\n",
    "        predicted_action = policy(obs)\n",
    "        actions.append(predicted_action.data.numpy())\n",
    "    return actions\n",
    "\n",
    "def dagger(expert_policy, student_policy, n_dagger_iterations, env=env_flagrun_without_rendering):\n",
    "    '''\n",
    "    Given an expert demonstrator and a student policy, perform\n",
    "    n iterations of DAGGER.\n",
    "    \n",
    "    '''\n",
    "    # collect initial expert demonstrations\n",
    "    n=10\n",
    "    print('Rolling Out Expert')\n",
    "    expert_rollout_data = rollout_for_n_episodes(n=10,\n",
    "                                            policy = expert_policy,\n",
    "                                            env = env_flagrun_without_rendering,\n",
    "                                            render=False)\n",
    "    print('Training Student')\n",
    "    # train initial student model with behavioural cloning\n",
    "    training_data = expert_rollout_data\n",
    "    trained_student = train_model(student_policy, expert_rollout_data)\n",
    "    \n",
    "    for i in range(n_dagger_iterations):\n",
    "        print('Iteration', i, 'of DAGGER')\n",
    "        \n",
    "        # rollout student model (renders by default! you can change this if you don't want to render)\n",
    "        student_rollout_data = rollout_for_n_episodes(n=10,\n",
    "                                            policy = student_policy,\n",
    "                                            env = env_flagrun_without_rendering,\n",
    "                                            render=False)\n",
    "        \n",
    "        # evaluate expert actions on student's trajectories and add to dataset\n",
    "        expert_corrections = evaluate_expert(expert_policy, student_rollout_data['observations'])\n",
    "        training_data = {'observations': training_data['observations'] + student_rollout_data['observations'],\n",
    "                         'actions':      training_data['actions']      + expert_corrections}\n",
    "        # train student model with behavioural cloning\n",
    "        student_policy = StudentPolicy(env_flagrun_with_rendering.observation_space,\n",
    "                               env_flagrun_with_rendering.action_space)  \n",
    "        student_policy =  train_model(student_policy, training_data, num_epochs = 500)\n",
    "        \n",
    "    return student_policy\n",
    "\n",
    "\n",
    "# instantiate a new untrained student policy\n",
    "torch.manual_seed(0)\n",
    "student_policy = StudentPolicy(env_flagrun_with_rendering.observation_space,\n",
    "                               env_flagrun_with_rendering.action_space)  \n",
    "\n",
    "dagger(flagrun_expert,\n",
    "       student_policy,\n",
    "       n_dagger_iterations = 2,\n",
    "       env = env_flagrun_without_rendering)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Watch the DAGGER-trained student policy\n",
    "\n",
    "You should be able to see the agent stand up! Remember you can click+drag on the humanoid in the GUI to knock it over and see how it recovers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view the trained student policy (it should run, but fall over!)\n",
    "rollout_for_n_episodes(n = 5,\n",
    "                       policy = student_policy,\n",
    "                       env = env_flagrun_with_rendering,\n",
    "                       render=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Train the Agent with the DAGGER Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Explore\n",
    "\n",
    "In this exercise, we have implemented the behavioural cloning and DAGGER algorithms, and demonstrated how to train policies using imitation learning that can *recover from their mistakes!*.\n",
    "\n",
    "To continue your learning, you are encouraged to complete any (or all!) of the following tasks:\n",
    "\n",
    "### 5.1 Adversarial Environments\n",
    "\n",
    "Another way to teach student policies to recover is to make an environment so difficult that it *forces the expert to fail!*\n",
    "\n",
    "You can then see what they do to recover, and use this as training data.\n",
    "\n",
    "Try this out! The `HumanoidFlagrunHarderBulletEnv-v0` environment is the same as before, except fast blocks are thrown directly at the humanoid to knock it over. Replace the environment in section 1.2. The expert will fall over a lot!\n",
    "\n",
    "Now you should be able to train a flagrun agent that can recover from a fall with basic behavioural cloning, without needing DAGGER! Try it!\n",
    "\n",
    "### 5.2 Imitation as an objective during RL training\n",
    "\n",
    "We've considered two basic approaches to imitation learning that don't require intensive RL algotithms like you have used in previous sections, instead relying upon having direct access to the observation space and action space of an expert demonstrator (e.g. recorded commands from a car's data bus as a human demonstrator drives around!).\n",
    "\n",
    "These approaches won't work in other situations. For instance, imagine you're trying to train a [humanoid robot](https://www.youtube.com/watch?v=LikxFZZO2sk) with imitation learning. Where is the expert data? We can watch a person running, but we can't record their joint torques! (And even if we could, they wouldn't transfer to a robot.)\n",
    "\n",
    "More powerful imitation learning approaches can be used in this situation to *guide* the reinforcement learning process.\n",
    "\n",
    "One examples is [DEEPMIMIC](https://bair.berkeley.edu/blog/2018/04/10/virtual-stuntman/), which combines a motion-imitation objective with the task objective, and ends up producing motions that are far more natural-looking than with [traditional RL approaches](https://www.youtube.com/watch?v=hx_bgoTF7bs).\n",
    "\n",
    "Read about this approach. Can you get the \n",
    "\n",
    "\n",
    "### 5.3 (VERY HARD) Hierarchical Imitation Learning\n",
    "\n",
    "The flagrun environment is *multitask*, which means that .\n",
    "\n",
    "Can you use the expert data to train an agent ? This has important implications for robotics research.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
