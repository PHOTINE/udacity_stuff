{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imitation Learning: Behavioural Cloning and the DAGGER Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import the Necessary Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add parent dir to find package. Only needed for source code build, pip install doesn't need it.\n",
    "import os, inspect\n",
    "currentdir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "parentdir = os.path.dirname(os.path.dirname(currentdir))\n",
    "os.sys.path.insert(0,parentdir)\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import pybullet_envs\n",
    "import os.path\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Instantiate the Environment, Agent, and Expert Demonstrator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flagrun_expert_demonstrator import *\n",
    "\n",
    "gui = True\n",
    "env = gym.make(\"HumanoidFlagrunBulletEnv-v0\")\n",
    "if (gui):\n",
    "  env.render(mode=\"human\")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as f \n",
    "                            \n",
    "class StudentPolicy(nn.Module):\n",
    "    \"Simple multi-layer perceptron policy, no internal state\"\n",
    "    def __init__(self, observation_space, action_space):\n",
    "        super(StudentPolicy, self).__init__()\n",
    "        self.weights_dense1 = nn.Linear(observation_space.shape[0], 256) \n",
    "        self.weights_dense2 = nn.Linear(256, 128) \n",
    "        self.weights_dense_final = nn.Linear(128, action_space.shape[0]) \n",
    "\n",
    "        torch.nn.init.xavier_uniform_(self.weights_dense1.weight)\n",
    "        torch.nn.init.xavier_uniform_(self.weights_dense2.weight)\n",
    "        torch.nn.init.xavier_uniform_(self.weights_dense_final.weight)\n",
    "        \n",
    "        self.weights_dense1.bias.data.fill_(0.01)\n",
    "        self.weights_dense2.bias.data.fill_(0.01)\n",
    "        self.weights_dense_final.bias.data.fill_(0.01)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = f.relu(self.weights_dense1(x))\n",
    "        x = f.relu(self.weights_dense2(x))\n",
    "        x = self.weights_dense_final(x)\n",
    "        return x\n",
    "\n",
    "                            \n",
    "        \n",
    "def rollout_for_one_episode(policy= ExpertPolicy(env.observation_space, env.action_space)):\n",
    "    '''\n",
    "    \n",
    "    Rollout a particular policy for a single episode.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    rollout_data = {'observations':[], 'actions':[]}\n",
    "    pi = policy\n",
    "    \n",
    "    frame = 0\n",
    "    score = 0\n",
    "    restart_delay = 0\n",
    "    obs = env.reset()\n",
    "    from itertools import count\n",
    "    for t in count():\n",
    "        rollout_data['observations'].append(obs)\n",
    "        a = pi(torch.Tensor(obs)).data.numpy()\n",
    "        import pdb\n",
    "        rollout_data['actions'].append(a)\n",
    "        obs, r, done, _ = env.step(a)\n",
    "        score += r\n",
    "        frame += 1\n",
    "        if (gui):\n",
    "          time.sleep(1./60)\n",
    "\n",
    "        still_open = env.render(\"human\")\n",
    "\n",
    "        if still_open==False:\n",
    "            return\n",
    "        if not done: continue\n",
    "        if restart_delay==0:\n",
    "            print(\"score=%0.2f in %i frames\" % (score, frame))\n",
    "            if still_open!=True:      # not True in multiplayer or non-Roboschool environment\n",
    "                break\n",
    "            restart_delay = 60*2  # 2 sec at 60 fps\n",
    "        restart_delay -= 1\n",
    "        if restart_delay==0: break\n",
    "    return rollout_data\n",
    "\n",
    "\n",
    "def rollout_for_n_episodes(n, policy= ExpertPolicy(env.observation_space, env.action_space)):\n",
    "    '''\n",
    "    Rollout a particular policy for a n episodes.\n",
    "    \n",
    "    '''\n",
    "    rollout_data = {'observations':[], 'actions':[]}\n",
    "    for i in range(n):\n",
    "        print('episode', i)\n",
    "        recent_rollout_data = rollout_for_one_episode(policy)\n",
    "        rollout_data['observations'].extend(recent_rollout_data['observations'])\n",
    "        rollout_data['actions'].extend(recent_rollout_data['actions'])\n",
    "    return rollout_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train the Agent with Behavioural Cloning\n",
    "\n",
    "Press 'w' in the pybullet GUI to turn on wireframe mode to render more quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils import data\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as f\n",
    "\n",
    "class Dataset(data.Dataset):\n",
    "  'Characterizes a dataset for PyTorch'\n",
    "  def __init__(self, X, Y):\n",
    "        'Initialization'\n",
    "        self.X=X\n",
    "        self.Y=Y\n",
    "\n",
    "  def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return len(self.X)\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "        # Select sample\n",
    "        X = self.X[index]\n",
    "        Y = self.Y[index]\n",
    "        return X, Y\n",
    "\n",
    "def train_model(policy, training_data):\n",
    "    '''\n",
    "    Given a dict of training data, train a policy network\n",
    "    using supervised learning.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    dataset = Dataset(training_data['observations'], training_data['actions'])\n",
    "    dataloader = data.DataLoader(dataset, batch_size = 128)\n",
    "    \n",
    "    optimizer = optim.Adam(policy.parameters(), lr=1e-3)\n",
    "    mse = nn.MSELoss()\n",
    "    for ne in range(100):\n",
    "        for obs, act in dataloader:\n",
    "\n",
    "            obs = Variable(obs)\n",
    "            act = Variable(act)\n",
    "\n",
    "            policy.zero_grad()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            predicted_action = policy(obs)\n",
    "            loss = mse(predicted_action, act.float())\n",
    "\n",
    "            loss.backward(retain_graph=True)\n",
    "            optimizer.step()\n",
    "\n",
    "            print(\"Epoch: {}, Total loss: {}\".format(ne, loss))\n",
    "    return policy\n",
    "            \n",
    "def evaluate_model(policy, data):\n",
    "    '''\n",
    "    Evaluate a policy on a list of recorded observations.\n",
    "    '''\n",
    "    actions = []\n",
    "    for obs in data:\n",
    "        obs = Variable(torch.Tensor(obs))\n",
    "        predicted_action = policy(obs)\n",
    "        actions.append(predicted_action.data.numpy())\n",
    "    return actions\n",
    "\n",
    "def behavioural_cloning(expert_policy, student_policy):\n",
    "    '''\n",
    "    Given an expert demonstrator and a student policy, perform\n",
    "    n iterations of dagger.\n",
    "    \n",
    "    '''\n",
    "    # collect initial expert demonstrations\n",
    "    n=10\n",
    "    print('Rolling Out Expert')\n",
    "    expert_rollout_data = rollout_for_n_episodes(10, expert_policy)\n",
    "    # train initial student model with behavioural cloning\n",
    "    student_policy = train_model(student_policy, expert_rollout_data)\n",
    "    return student_policy\n",
    "\n",
    "\n",
    "expert_policy= ExpertPolicy(env.observation_space, env.action_space)\n",
    "student_policy = StudentPolicy(env.observation_space, env.action_space)   \n",
    "\n",
    "behavioural_cloning(expert_policy, student_policy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_rollout_data = rollout_for_n_episodes(10, student_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train the Agent with the DAGGER Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dagger(expert_policy, student_policy, n_dagger_iterations):\n",
    "    '''\n",
    "    Given an expert demonstrator and a student policy, perform\n",
    "    n iterations of dagger.\n",
    "    \n",
    "    '''\n",
    "    # collect initial expert demonstrations\n",
    "    n=10\n",
    "    expert_rollout_data = rollout_for_n_episodes(n, expert_policy)\n",
    "    # train initial student model with behavioural cloning\n",
    "    trained_student = train_model(student_policy, expert_rollout_data)\n",
    "    \n",
    "    for i in range(n_dagger_iterations):\n",
    "        # rollout student model\n",
    "        student_rollout_data = rollout_for_n_episodes(3, student_policy)\n",
    "        # evaluate expert actions on student's trajectories and add to dataset\n",
    "        expert_corrections = evaluate_model(expert_policy, student_rollout_data['observations'])\n",
    "        training_data = {'observations': expert_rollout_data['observations'] + student_rollout_data['observations'],\n",
    "                         'actions':      expert_rollout_data['actions']      + expert_corrections}\n",
    "        # train student model with behavioural cloning\n",
    "        student_policy =  train_model(student_policy, training_data)\n",
    "        \n",
    "    return student_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expert_policy= ExpertPolicy(env.observation_space, env.action_space)\n",
    "student_policy = StudentPolicy(env.observation_space, env.action_space)   \n",
    "dagger(expert_policy, student_policy, n_dagger_iterations = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Explore\n",
    "\n",
    "In this exercise, we have implemented the behavioural cloning and DAGGER algorithms, and demonstrated how to use them to solve a pybullet Gym environment. To continue your learning, you are encouraged to complete any (or all!) of the following tasks:\n",
    "\n",
    "- Plot the  behavioural cloning student policy's average reward for a variety of numbers of episodes of expert data, and compare to the expert.\n",
    "- Change the environment to 'HumanoidFlagrunHarderBulletEnv-v0' environment, and run behavioural cloning on 10 episodes of expert data. Watch the visualization. Does behavioural cloning work better than for previous environment? Why?\n",
    "- Try and reduce the amount of expert data needed for Dagger to work on 'HumanoidFlagrunHarderBulletEnv-v0'. Can you reach an average reward of 500 over ten episodes, using only a total of 100 frames of expert data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "solutions: \n",
    "\n",
    "\n",
    "- reward increases slowly...\n",
    "- Works better because the blocks push it off-distribution so you widen the expert's trajectory distribution (it knows how to correct).\n",
    "- trick is to (1) run loads of iterations of dagger, (2) use temporally-distant examples."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
